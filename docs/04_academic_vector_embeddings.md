# Vector Embeddings in NLP

 Surveys embedding models, evaluation methods, challenges (bias, polysemy), and future directions.

Vector embeddings map discrete tokens (words, sentences, documents) into continuous vector spaces that capture semantic relationships.

## Common Models
- Word2Vec
- GloVe
- fastText
- Transformer-based embeddings

## Evaluation
Intrinsic: analogy tasks, similarity scores.
Extrinsic: downstream performance in classification, retrieval.

## Challenges
Bias, polysemy, domain shift, and high-dimensional sparsity.

## Future Directions
Contextual, multimodal, and instruction-tuned representations.
